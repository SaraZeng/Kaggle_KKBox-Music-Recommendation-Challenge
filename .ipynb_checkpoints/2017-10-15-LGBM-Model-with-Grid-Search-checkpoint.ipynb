{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 985.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the first 99 rows for demo\n",
    "train = pd.read_csv('../train.csv', nrows=99)\n",
    "test = pd.read_csv('../test.csv',nrows=99)\n",
    "songs = pd.read_csv('../songs.csv')\n",
    "members = pd.read_csv('../members.csv')\n",
    "\n",
    "# Merge datasets with song attributes\n",
    "song_cols = ['song_id', 'artist_name', 'genre_ids', 'song_length', 'language']\n",
    "train = train.merge(songs[song_cols], on='song_id', how='left')\n",
    "test = test.merge(songs[song_cols], on='song_id', how='left')\n",
    "\n",
    "# Merge datasets with member features\n",
    "members['registration_year'] = members['registration_init_time'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['registration_month'] = members['registration_init_time'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['registration_date'] = members['registration_init_time'].apply(lambda x: int(str(x)[6:8]))\n",
    "\n",
    "members['expiration_year'] = members['expiration_date'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['expiration_month'] = members['expiration_date'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['expiration_date'] = members['expiration_date'].apply(lambda x: int(str(x)[6:8]))\n",
    "members = members.drop(['registration_init_time'], axis=1)\n",
    "\n",
    "members_cols = members.columns\n",
    "train = train.merge(members[members_cols], on='msno', how='left')\n",
    "test = test.merge(members[members_cols], on='msno', how='left')\n",
    "\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "del members, songs; gc.collect();\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.drop(['target'], axis=1))\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['max_depth', 'reg_lambda', 'silent', 'learning_rate', 'objective', 'n_jobs', 'subsample_for_bin', 'subsample', 'subsample_freq', 'num_leaves', 'boosting_type', 'min_split_gain', 'n_estimators', 'reg_alpha', 'colsample_bytree', 'max_bin', 'min_child_samples', 'min_child_weight', 'random_state'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'application': 'binary', # for binary classification\n",
    "#     'num_class' : 1, # used for multi-classes\n",
    "    'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "    'num_iterations': 100, \n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 62,\n",
    "    'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "    'max_depth': -1, # <0 means no limit\n",
    "    'max_bin': 510, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "    'lambda_l1': 5, # L1 regularization\n",
    "    'lambda_l2': 10, # L2 regularization\n",
    "    'metric' : 'binary_error',\n",
    "    'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "    'subsample': 1, # subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
    "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
    "}\n",
    "\n",
    "# Initiate classifier to use\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          objective = 'binary', \n",
    "          n_jobs = 5, \n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'])\n",
    "\n",
    "# To view the default model parameters:\n",
    "mdl.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3456 candidates, totalling 13824 fits\n"
     ]
    }
   ],
   "source": [
    "gridParams = {\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'n_estimators': [8,16,24],\n",
    "    'num_leaves': [6,8,12,16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n",
    "    'objective' : ['binary'],\n",
    "    'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n",
    "    'random_state' : [500],\n",
    "    'colsample_bytree' : [0.64, 0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=1, cv=4, n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['colsample_bytree'] = grid.best_params_['colsample_bytree']\n",
    "params['learning_rate'] = grid.best_params_['learning_rate'] \n",
    "params['max_bin'] = grid.best_params_['max_bin']\n",
    "params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "params['reg_alpha'] = grid.best_params_['reg_alpha']\n",
    "params['reg_lambda'] = grid.best_params_['reg_lambda']\n",
    "params['subsample'] = grid.best_params_['subsample']\n",
    "\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "ids = test['id'].values\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state = 12)\n",
    "    \n",
    "del X, y; gc.collect();\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_valid = lgb.Dataset(X_valid, label=y_valid) \n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1000, valid_sets=watchlist, early_stopping_rounds=50, verbose_eval=4)\n",
    "\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "submName = strftime(\"%Y%m%d%H%M%S\", gmtime()) + '_submission.csv.gz'\n",
    "subm.to_csv(submName, compression = 'gzip', index=False, float_format = '%.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
